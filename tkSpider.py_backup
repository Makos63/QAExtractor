import scrapy
from scrapy.http import Request
from webscalper.items import WebscalperItem


class ArticlesSpider(scrapy.Spider):
    name = 'Articles'
    allowed_domains = ['www.tk.de']
    start_urls = ('https://www.tk.de/vertriebspartner/faq-2059618')
    def start_requests(self):

        request = Request('https://www.tk.de/vertriebspartner/faq-2059618', cookies={'store_language': 'en'}, callback=self.parse_article_page)
        yield request

    def parse_article_page(self,response):
        item=WebscalperItem()
        # Gets HTML content where the article links are stored
        #content=response.xpath('//div[@class="t-faq"]'
        #                       '//div[@class="m-verteilerliste"]'
        #                       '//div[@class="m-verteilerliste__link-container"]')
        content = response.xpath(#'//main[@class="u-viewport"]'
                                 #'//div[@class="t-faq "]'
                                 '//section[@class="c-navigationsliste "]')
                                 #'//div[@class="m-verteilerliste m-verteilerliste--questionlist m-verteilerliste--1columns m-verteilerliste--icon is-loaded is-active"]')
                                 #'//nav[@class="m-verteilerliste__container"]'
                                 #'//ul[@class="m-verteilerliste__links "]')


        # Loops through the each and every article link in HTML 'content'
        print("content.xpath")
        print(content)
        #for article_li in content.xpath('.//li'):
        for article_a in content.xpath('.//a'):
            # Extracts the href info of the link to store in scrapy item
            item['url'] = article_a.xpath('.//@href').extract_first()
            item['url'] = "www.tk.de"+item['url']
            yield(item)

    #def parse(self, response):
    pass


